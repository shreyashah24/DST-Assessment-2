{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we sought to focus on the problem of topic modelling, specifically using the BBC News dataset, where we looked to create a model that could sort an article based on its content into a category, namely \"business\", \"entertainment\", \"politics\", \"sport\" or \"tech\".\n",
    "\n",
    "As for the choice of dataset, there were only a few readily available datasets which contained both some information about the article along with the category. Another [dataset on Kaggle](https://www.kaggle.com/datasets/rmisra/news-category-dataset?fbclid=IwAR36vejorgJkTwzU1HrTC1NqJ060fbIRjvA_JIVyH9L9IO-yEqiCG_zPIlI) that we looked at had a large number of entries, in the hundreds of thousands, with a suitable choice of category, however this dataset only provided the headlines or just short abstracts of the articles. In fact, all the other datasets we found similarly only provided a small part of the article. So, although the BBC News dataset only had around 2000 entries, we chose this dataset mainly due to the amount of content in each of the articles, as the full article was provided in each case. We determined that the extra information provided by the full articles would be better data to analyse than just the headlines of some articles, justifying our choice of the BBC News dataset. This would theoretically enable us to input another unknown article and obtain its predicted topic based on the content of the article, rather than the headline which is not necessarily wholly representative of the content in the article itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the data, we decided to tokenise the data and remove stopwords. Tokenising and removing the stopwords did not prove to be a large issue, the main aspect that we had to take particular care of was determining the stopwords themselves, and always keeping in mind what a stopword in fact signifies. In the end, we manually selected the stopwords found in stopwords.txt, which removed over half of the words in all the articles, while keeping words that would particularly indicate the topic of an article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models all used a bag of words technique in some form. Namely, the models were Naive Bayes, Multinomial Logistic Regression and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "In the end, the Multinomial Logistic Regression model did not provide the desired results, always giving \"business\" as the predicted category. We searched for the error but could not find it in the timeframe, so we decided to focus on the remaining two models.\n",
    "\n",
    "The Latent Dirichlet Allocation model did provide the intended visual results, as shown in the analysis of the interactive plot in 05-LDA.ipynb, the previous file in this report. It clearly separated words into their separate categories when the categories were distinct, while showing that topics such as \"entertainment\" and \"tech\" did have some crossover, such as when talking about topics such as social media, television, radio, etc.\n",
    "\n",
    "However, when it came to predicting topics for articles from the test dataset, the prediction only obtained an accuracy score of 23% (correct predictions/total number of entries in the dataset). This suggests that there was potentially a problem with taking the analysis from the model and converting it into topic prediction. Again, we tried to search for the error here, but this was not possible in the timeframe.\n",
    "\n",
    "As for the Naive Bayes model, this worked much better than expected (though not so surprising that it raised any alarms), as it obtained an accuracy score of 95.07% upon predicting the topics of the test set. Therefore, we deem this model, the Naive Bayes, to be successful, and the best performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we did end up with a highly successful implementation, however to improve, we would, time permitting of course, have liked to have looked more deeply into the errors we ran into when trying to implement the Multinomial Logistic Regression and LDA models, as it would be useful to have another model to compare against (although the comparison and competition element of this project was not very important).\n",
    "\n",
    "We successfully found and chose a dataset that fit our specifications and we justified our choice of dataset. This enabled us to work with a very suitable dataset for our purposes, and thus had no real difficulties in using this dataset. We would have liked to have had more articles to use (an article scraping approach was looked into but quickly abandoned), however we found that the number of articles to train and test off were sufficient, as proven by the success of the Naive Bayes model. In the future, and with more time, we could have created a scraper to obtain more articles, and if this would prove to be too much for a home computer to process, we could have attempted to use a high-performance computing system such as BlueCrystal in order to process more articles.\n",
    "\n",
    "Though satisfied with the implementation of our successful model, there are still these questions raised, which would be of great interest to explore in a project with a longer timeframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
